{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Importing necessary libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "acec0405d9ff97f6"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T13:46:59.157449100Z",
     "start_time": "2023-09-30T13:46:59.091434500Z"
    }
   },
   "id": "581ec6f26f73a735"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Problem 1 (a)\n",
    "$f(x) = x^{T}Ax + b$ \\\n",
    "$\\Rightarrow \\nabla f(x) = 2Ax$ \\\n",
    "where $A = \\begin{bmatrix} 2 & -1 & -1 \\\\ -1 & 2 & 0 \\\\ -1 & 0 & 1 \\end{bmatrix}$, $b = \\begin{bmatrix} 1 \\end{bmatrix}$ \n",
    "By setting the gradient to zero \n",
    "$\\Rightarrow 2Ax = 0$ \\\n",
    "$\\Rightarrow x = 0$ "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "396bf7a213648b1a"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimizer using gradient descent: [[0.00023901]\n",
      " [0.00013264]\n",
      " [0.00029804]]\n",
      "Value of f_a at minimizer: 1.000000032392033\n",
      "Minimizer using gradient zero: [[0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "Value of f_a at minimizer using gradient zero: 1.0\n"
     ]
    }
   ],
   "source": [
    "A_a = np.matrix([[2, -1, -1], [-1, 2, 0], [-1, 0, 1]])\n",
    "b_a = np.matrix([[1]])\n",
    "\n",
    "def f_a(x):\n",
    "    return x.T @ A_a @ x + b_a\n",
    "\n",
    "def gradient_f_a(x):\n",
    "    return 2* A_a @ x\n",
    "\n",
    "def gradient_descent(x0, learning_rate, num_iterations):\n",
    "    x = x0\n",
    "    for i in range(num_iterations):\n",
    "        x = x - learning_rate * gradient_f_a(x)\n",
    "    return x\n",
    "\n",
    "N = 1000\n",
    "t = 0.02\n",
    "\n",
    "x0 = np.random.rand(3,1)\n",
    "result_gradient_descent = gradient_descent(x0, t, N)\n",
    "print(\"Minimizer using gradient descent:\", result_gradient_descent)\n",
    "print(\"Value of f_a at minimizer:\", f_a(result_gradient_descent)[0,0])\n",
    "result_gradient_zero = (1/2) * A_a.I @ np.matrix([[0],[0],[0]])\n",
    "print(\"Minimizer using gradient zero:\", result_gradient_zero)\n",
    "print(\"Value of f_a at minimizer using gradient zero:\", f_a(result_gradient_zero)[0,0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T13:46:59.173231Z",
     "start_time": "2023-09-30T13:46:59.157449100Z"
    }
   },
   "id": "70eb247def381c74"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Problem 1 (b)\n",
    "$f(x) = ||Ax - b||^{2}$ \\\n",
    "$\\Rightarrow \\nabla f(x) = 2A^{T}(Ax - b)$ \\\n",
    "where $A = \\begin{bmatrix} 1 & 2 \\\\ 2 & 4 \\\\ 3 & 1 \\end{bmatrix}$, $b = \\begin{bmatrix} 1 \\\\ 3 \\\\ 1 \\end{bmatrix}$\n",
    "By setting the gradient to zero\n",
    "$\\Rightarrow 2A^{T}(Ax - b) = 0$ \\\n",
    "$\\Rightarrow A^{T}(Ax - b) = 0$ \\\n",
    "$\\Rightarrow A^{T}Ax = A^{T}b$ \\\n",
    "$\\Rightarrow x = (A^{T}A)^{-1}A^{T}b$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a44cf8d9392d040e"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimizer using gradient descent: [[0.12]\n",
      " [0.64]]\n",
      "Value of f_b at minimizer: 0.20000000000000107\n",
      "Minimizer using gradient zero: [[0.12]\n",
      " [0.64]]\n",
      "Value of f_b at minimizer using gradient zero: 0.20000000000000107\n"
     ]
    }
   ],
   "source": [
    "A_b = np.matrix([[1,2],[2,4],[3,1]])\n",
    "b_b = np.matrix([[1],[3],[1]])\n",
    "\n",
    "def f_b(x):\n",
    "    return x.T @ A_b.T @ A_b @ x - b_b.T @ A_b @ x - x.T @ A_b.T @ b_b + b_b.T @ b_b\n",
    "\n",
    "def gradient_f_b(x):\n",
    "    return 2* A_b.T @ (A_b @ x - b_b)\n",
    "\n",
    "def gradient_descent(x0, learning_rate, num_iterations):\n",
    "    x = x0\n",
    "    for i in range(num_iterations):\n",
    "        x = x - learning_rate * gradient_f_b(x)\n",
    "    return x\n",
    "\n",
    "N = 1000\n",
    "t = 0.02\n",
    "x0 = np.random.rand(2,1)\n",
    "result_gradient_descent = gradient_descent(x0, t, N)\n",
    "print(\"Minimizer using gradient descent:\", result_gradient_descent)\n",
    "print(\"Value of f_b at minimizer:\", f_b(result_gradient_descent)[0,0])\n",
    "result_gradient_zero = (A_b.T @ A_b).I @ A_b.T @ b_b\n",
    "print(\"Minimizer using gradient zero:\", result_gradient_zero)\n",
    "print(\"Value of f_b at minimizer using gradient zero:\", f_b(result_gradient_zero)[0,0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T13:46:59.217427500Z",
     "start_time": "2023-09-30T13:46:59.173231Z"
    }
   },
   "id": "136320cb1037b19a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Problem 1 (c)\n",
    "$f(x) = ||Ax - b||^{2}$ \\\n",
    "$\\Rightarrow \\nabla f(x) = 2A^{T}(Ax - b)$ \\\n",
    "where $A = \\begin{bmatrix} 1 & 2 & 1 \\\\ 2 & 4 & 2 \\\\ 3 & 1 & 9 \\\\ 4 & 1 & 0 \\\\ 2 & 1 & 4 \\end{bmatrix}$, $b = \\begin{bmatrix} 1 \\\\ 3 \\\\ 1 \\\\ 0 \\\\ 9 \\end{bmatrix}$\n",
    "By setting the gradient to zero\n",
    "$\\Rightarrow 2A^{T}(Ax - b) = 0$ \\\n",
    "$\\Rightarrow A^{T}(Ax - b) = 0$ \\\n",
    "$\\Rightarrow A^{T}Ax = A^{T}b$ \\\n",
    "$\\Rightarrow x = (A^{T}A)^{-1}A^{T}b$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5cc5f5ec080838d4"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimizer using gradient descent: [[0.05744939]\n",
      " [0.65686105]\n",
      " [0.33915902]]\n",
      "Value of f_c at minimizer: 56.99048278248832\n",
      "Minimizer using gradient zero: [[0.05744939]\n",
      " [0.65686105]\n",
      " [0.33915902]]\n",
      "Value of f_c at minimizer using gradient zero: 56.99048278248832\n"
     ]
    }
   ],
   "source": [
    "A_c = np.matrix([[1,2,1],[2,4,2],[3,1,9],[4,1,0],[2,1,4]])\n",
    "b_c = np.matrix([[1],[3],[1],[0],[9]])\n",
    "\n",
    "def f_c(x):\n",
    "    return x.T @ A_c.T @ A_c @ x - b_c.T @ A_c @ x - x.T @ A_c.T @ b_c + b_c.T @ b_c\n",
    "\n",
    "def gradient_f_c(x):\n",
    "    return 2* A_c.T @ (A_c @ x - b_c)\n",
    "\n",
    "def gradient_descent(x0, learning_rate, num_iterations):\n",
    "    x = x0\n",
    "    for i in range(num_iterations):\n",
    "        x = x - learning_rate * gradient_f_c(x)\n",
    "    return x\n",
    "\n",
    "N = 1000\n",
    "t = 0.002 # learning rate is made smaller than previous problems due to out of bound errors\n",
    "x0 = np.random.rand(3,1)\n",
    "result_gradient_descent = gradient_descent(x0, t, N)\n",
    "print(\"Minimizer using gradient descent:\", result_gradient_descent)\n",
    "print(\"Value of f_c at minimizer:\", f_c(result_gradient_descent)[0,0])\n",
    "result_gradient_zero = (A_c.T @ A_c).I @ A_c.T @ b_c\n",
    "print(\"Minimizer using gradient zero:\", result_gradient_zero)\n",
    "print(\"Value of f_c at minimizer using gradient zero:\", f_c(result_gradient_zero)[0,0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-30T13:46:59.217427500Z",
     "start_time": "2023-09-30T13:46:59.195582700Z"
    }
   },
   "id": "ff42759223a214eb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
