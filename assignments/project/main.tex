\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{setspace}
\setstretch{1.25}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{wasysym}
\usepackage{caption2}
\newcommand{\BibTeX}{\textrm{B \kern -.05em \textsc{i \kern -.025em b} \kern -.08em
T \kern -.1667em \lower .7ex \hbox{E} \kern -.125emX}}
\begin{document}

    \title{KMeans Kernel Classifier\\
    {\footnotesize \textsuperscript {}Course: Math Behind ML }
    }

    \author{\IEEEauthorblockN{Karthik Kurugodu}
    \IEEEauthorblockA{\textit{B.Tech Mathematics and Computing} \\
    {Indian Institute of Technology Hyderabad}\\
    ma20btech11008@iith.ac.in}
    \and
    \IEEEauthorblockN{Nikhil Kongara}
    \IEEEauthorblockA{\textit{B.Tech Mathematics and Computing} \\
    {Indian Institute of Technology Hyderabad}\\
    ma20btech11011@iith.ac.in}
    }

    \maketitle

    \begin{abstract}
        The least squares SVM is a kernel method for non-linear regression and classification tasks.
        Here we combine KMeans clustering with the least squares SVM. First KMeans clustering is used to extract a set of representative vectors for each class, and then LS-SVM uses these representative vectors as a training dataset for the classification task
    \end{abstract}


    \section{Introduction}\label{sec:introduction}
    The kernel methods transform a given non-linear problem into a linear one by using a similarity kernel function $\Omega(x,x\prime)$.
    It is a similarity function defined over pairs of input data points $(x, x\prime)$.
    This way the input data is mapped into a higher dimensional feature space  $\phi(x)$, where the inner product $ \langle\cdot\;,\;\cdot\rangle\ $ can be calculated using Mercer's condition:
    \begin{align}
        \Omega(x,x\prime) = \langle x \;,\; x\prime\rangle\
    \end{align}
    Consider $\chi = \{x_{n} | n=1,\ldots,N\}$ as training dataset. \\
    \textbf{Representer theorem:} Any non-linear function $f : \chi \longrightarrow \mathbb{R}$ can be expressed as linear combination of kernel products on training dataset which was mentioned above earlier.
    \begin{align}
        f(x) = \sum_{n=1}^{N} a_{n}\Omega(x,x_{n})
    \end{align}
    Time complexity of LS-SVM is O(N$^3$) where N is size of the training dataset which is too high and makes it unsuitable for large dataset.
    So for this reason we use KMeans clustering to extract a set of representative vectors for each class, and then LS-SVM uses these representative vectors as a training dataset for the classification task.
    This way we can reduce the time complexity of LS-SVM to O(K$^3$) where K is the number of clusters.
    These representative vectors are called as \textbf{centroids}.
    These are then used by LS-SVM to classify the test data. \\
    This KMeans-LS-SVM method has some advantages:
    \begin{itemize}
        \item It is faster than LS-SVM\@.
        \item It is more robust.
        \item It is very easy to implement.
    \end{itemize}


    \section{Kernel LS-SVM Classifer}\label{sec:kernel-ls-svm-classifer}
    We already know that in binary classification, kernel SVM method constructs a hyperplane with the maximal margin
    between the two classes in feature space $ \phi(x) $.
    This can be represented as convex quadratic programming problem
    involving inequality constraints. \\
    The kernel LS-SVM simplifies the optimization problem by
    considering equality constraints only, such that solution is obtained by solving a system of linear equations.
    Now this problem is similar to ridge regression problem which is formulated as follows:
    \begin{align}
        \min_{w,b} \frac{1}{2}w^{T}w + \frac{\gamma}{2}\sum_{n=1}^{N}(\hat{y}_{n} - w^{T}\phi(x_{n}) - b)^{2}
    \end{align}
    Assume that K classes are encoded using standard basis in $\mathbb{R}^{K}$, i.e, let $x_{i} \in C_{k}$, then output
    $ y_{i}$ is a vector with 1 in the $k^{th}$ position and 0 elsewhere:
    \begin{align}
        y_{ij} = \begin{cases}
                     1 & \text{if } x_{i} \in C_{j} \\
                     0 & \text{otherwise}
        \end{cases}
    \end{align}
    Consider input data $\{(x_{i},y{i}) | x_{i}\in\mathbb{R^{M}},y_{i}\in\mathbb{R^{K}}, i = 1,\ldots,N\}$ and the
    feature mapping function $\phi(x)$.
    The kernel LS-SVM is formulated as follows:
    \begin{align}
        \min_{w,b} S(w,b,\epsilon) = \frac{1}{2}\sum_{j=1}^{K}w_{j}^{T}w_{j} + \frac{\gamma}{2}\sum_{i=1}^{N}\sum_{j=1}^{K}(\epsilon_{ij})^{2}
    \end{align}
    subject to
    \begin{align}
        \langle \phi(x) \;,\; \omega_{j}  \rangle + b_{j} = y_{ij} - \epsilon_{ij}, i = 1,\ldots,N; j = 1,\ldots,K \\
        w_{j}^{T}\phi(x_{i}) + b_{j} = y_{ij} - \epsilon_{ij} , i = 1,\ldots,N; j = 1,\ldots,K
    \end{align}
    where $\epsilon_{ij} \geq 0$ are approximation errors, $b_{j}$ is bias coefficient, $w^{(j)}$ is the vector of
    weights corresponding to the $j^{th}$ class.
    The objective function $S$ is a sum of least squares errors and the
    regularization term.
    This regularization parameter $\gamma$ corresponds to a multi-dimensional version of the ridge
    regression problem.
    \doublespacing In the primal weight space the multi class classifier takes the form:
    \begin{align*}
        & x \in C_{k}, \Leftrightarrow k= arg \max_{j=1,\ldots,K} g_{j}(x) \\
        & \text{where } g_{j}(x) = \frac{\exp(\langle \phi(x)\;,\; w^{(j)})\rangle + b_{j})}{\sum_{i=1}^{K} \exp(\langle \phi(x)\;,\; w^{(i)})\rangle + b_{i})}
    \end{align*}
    Here $g_{j}$ is the non-linear soft max function \\
    Now applying Lagrangian to $(5)$
    \begin{align*}
        &L(w,b,\epsilon,a) = S(w,b,\epsilon)\\
        &- \sum_{i=1}^{N} \sum_{j=1}^{K} a_{ij}[\langle \phi(x) \;,\; \omega_{j}  \rangle + b_{j} - y_{ij} + \epsilon_{ij}]
    \end{align*}
    where $a_{ij} \in \mathbb{Re}$ is the lagrange multiplier.
    Now applying KKT conditions:
    \begin{align}
        &\frac{{\partial L}}{{\partial w^{(j)}}} = 0 \implies w^{(j)} = \sum_{n=1}^{N}a_{nj}\phi(x_{n}) \\
        &\frac{{\partial L}}{{\partial b_{(j)}}} = 0 \implies \sum_{i=1}^{N}a_{ij} = 0 \\
        &\frac{{\partial L}}{{\partial \epsilon_{(ij)}}} = 0 \implies
        a_{ij} = \gamma \epsilon_{ij} \\
        &\frac{{\partial L}}{{\partial a_{(ij)}}} = 0 \implies
        \langle \phi(x) \;,\; \omega_{j}  \rangle + b_{j} - y_{ij} + \epsilon_{ij} = 0
    \end{align}
    \\
    Now from eq$(10)$ , eq$(12)$ and eq$(13)$:
    \begin{align}
        &\sum_{n=1}^{N} [\Omega(x_{i},x_{n}) + \gamma^{-1}\delta_{in}]a_{nj} + b{j} = y_{ij},
    \end{align}
    Here $\delta_{in}$ is the Kronecker delta function: where $\delta_{in} =1$ if $i=n$ and $0$ otherwise \\
    As you can see in eq$(14)$ there are $K$ independent system of equations with binary labels $y_{ij}$.
    Now each system can be written in the matrix form as follows:
    \begin{align}
        \begin{bmatrix}
            0 & u^{T}                 \\
            u & \Omega + \gamma^{-1}I
        \end{bmatrix}
        \begin{bmatrix}
            b_{j} \\
            a^{(j)}
        \end{bmatrix}
        =
        \begin{bmatrix}
            0 \\
            y_{j}
        \end{bmatrix}
        , j = 1,\ldots,K
    \end{align}
    Here $I_{N \times N}$ is the identity matrix, $u_{N \times 1} = [1,\ldots,1]^{T}$ is a vector of ones,
    $a^{(j)}_{N \times 1} = [a_{1j}, \ldots, a_{Nj}]^{T}$ is weights and $y_{j} = [y_{1j}, \ldots, y_{Nj}]^{T}$ is the
    vector of binary labels for the $j^{th}$ class.

    Each system has $N+1$ linear equations with $N+1$ unknowns.
    Each system has $N+1$ linear equations with $N+1$
    unknowns.
    \begin{align}
        \Theta = \begin{bmatrix}
                     0 & u^{T}                 \\
                     u & \Omega + \gamma^{-1}I
        \end{bmatrix}
    \end{align}
    All the $K$ systems can be written as:
    \begin{align}
        \Theta W = Z
    \end{align}
    where
    \begin{align*}
        W_{(N+1) \times K} = \begin{bmatrix}
                                 b_{1}   & \ldots & b_{K}   \\
                                 a^{(1)} & \ldots & a^{(K)}
        \end{bmatrix}
        , Z_{(N+1) \times K} = \begin{bmatrix}
                                   0     & \ldots & 0     \\
                                   y_{1} & \ldots & y_{K}
        \end{bmatrix}
    \end{align*}
    Now once all the $K$ systems are solved, we consider multi-class classifier in dual space(from eq $(14)$) as follows:
    \begin{align*}
        &g_{j}(x) = \frac{\exp(\langle \phi(x)\;,\; w^{(j)})\rangle + b_{j})}{\sum_{i=1}^{K} \exp(\langle \phi(x)\;,\; w^{(i)})\rangle + b_{i})}\\
        &\text{From eq$(9)$ and eq$(10)$, we get:} \\
        &g_{j}(x) = \frac{\sum_{n=1}^{N}\exp(\Omega(x,x_{n})a_{nj} + b_{j})}{\sum_{i=1}^{K} \sum_{n=1}^{N}\exp(\Omega(x,x_{n})a_{ni} + b_{i})}
    \end{align*}
    Now our problem becomes:
    \begin{align}
        & x \in C_{k}, \Leftrightarrow k= arg \max_{j=1,\ldots,K} g_{j}(x)
    \end{align}
    \text{where}
    \begin{align*}
        & g_{j}(x) = \frac{\sum_{n=1}^{N}\exp(\Omega(x,x_{n})a_{nj} + b_{j})}{\sum_{i=1}^{K} \sum_{n=1}^{N}\exp(\Omega(x,x_{n})a_{ni} + b_{i})}
    \end{align*}
    \text{Here $g_{j}$ is the non-linear soft max function}


    \section{KMeans Clustering}\label{sec:kmeans-clustering}
    First we use KMeans clustering algorithm to extract a set of representative vectors for each class.
    Now this representative vectors will be passed into LS-SVM kernel model as training dataset.
    KMeans clustering algorithm is as follows:
    \begin{enumerate}
        \item Take $\{x_{i}^{k}|x_{i}^{k} \in \mathbb{R^{M}}, i=1,\ldots,N_{k}\}$ as training samples for class $C_{k}$ where
        $N_{k}$ is the number of training samples for the class $C_{k}$ and $N = \sum_{k=1}^{K}N_{k}$ is the total
        number of training samples.
        \item Take $\{\mu_{q}^{k}|\mu_{q}^{k} \in \mathbb{R^{M}}, q=1,\ldots,Q \}$ as initial centroids
        for class $C_{k}$ where $Q<N_{K}$ is the number of centroids for class $C_{k}$.
        \item Build a matrix $X_{k} = [x_{im}^{k}]_{N_{k} \times M}$ where each row is a training sample for class $C_{k}$.
        \item Build a matrix $\Xi_{k} = [\xi_{qm}^{k}]_{Q \times M}$ where each row is a randomly initialized centroid
        for class $C_{k}$.
        \item Let $R_{k} = X_{k}\Xi_{k}^{T} = [r_{iq}^{k}]_{N_{k}\times Q}$
        \item Let $\hat{R_{k}} = [\hat{r_{iq}^{k}}]_{N_{k}\times Q}$ be transformed sparse matrix of $R_{k}$ where: \\
        $\hat{r_{iq}^{k}} = \begin{cases}
                                1 & \text{if } q = \arg \max_{q}r_{iq}^{k} \\
                                0 & \text{otherwise}
        \end{cases}$
        $i = 1,\ldots,N_{k}$\\
        Each sample is assigned to the nearest centroid.
        \item $\hat{\Xi_{k}} = \hat{R_{k}^{T}}X_{k} = [\hat{\xi_{qm}^{k}}]_{Q \times M}$. \\
        This is the new set of centroids.
        \item Normalizing new set of centroids: \\
        $\hat{\xi_{q}^{k}} = \frac{\hat{\xi_{q}^{k}}}{|| \hat{\xi_{q}^{k}} ||} ,\\ q=1,\ldots,Q$
        \item Computing alignment deviation between new set and old set of centroids: \\
        $\delta = 1 - \frac{\sum_{q=1}^{Q}\langle \hat{\xi_{q}^{k}}\; \; \xi_{q}^{k} \rangle}{Q}$
        \item $\Xi_{k} = \hat{\Xi_{k}}$
        \item Repeat steps 5 to 10 until $\delta < \beta $ where $\beta$ is the tolerance.
        \item Return $\Xi_{k}$
    \end{enumerate}


    \section{KMeans Kernel LS-SVM Classifier}\label{sec:kmeans-kernel-ls-svm-classifier}
    After extracting a set of representative vectors for each class
    $C_{k}, k=1,\ldots,K$ using KMeans clustering,
    we pass these $KQ$ centroids into LS-SVM kernel model as training dataset. \\
    Training dataset for LS-SVM before KMeans clustering:
    \begin{align*}
        \{(x_{i}^{k},y_{i}^{k})| x_{i}^{k} \in \mathbb{R^{M}},y_{i}^{k} \in \mathbb{R^{K}}, i=1,\ldots,N\}
    \end{align*}
    Training dataset for LS-SVM after KMeans clustering:
    \begin{align*}
        \{(\xi_{q}^{k},y_{q}^{k})| \xi_{q}^{k} \in \mathbb{R^{M}},y_{q}^{k} \in \mathbb{R^{K}}, q=1,\ldots,KQ\}
    \end{align*}
    As you can see the training dataset size is reduced from $N$ to $KQ$ where $KQ<N$.

    Previously there were $N+1$ linear equations with $N+1$ unknowns and $O(N^3)$ time complexity.

    Now there are $KQ+1$ linear equations with $KQ+1$ unknowns and $O((KQ)^3)$ time complexity. \\
    As we discussed earlier our problem previously was:
    \begin{align*}
        & x \in C_{k}, \Leftrightarrow k= arg \max_{j=1,\ldots,K} g_{j}(x) \\
        & \text{where } g_{j}(x) = \frac{\sum_{n=1}^{N}\exp(\Omega(x,x_{n})a_{nj} + b_{j})}{\sum_{i=1}^{K} \sum_{n=1}^{N}\exp(\Omega(x,x_{n})a_{ni} + b_{i})}
    \end{align*}
    Now our problem becomes:
    \begin{align*}
        & x \in C_{k}, \Leftrightarrow k= arg \max_{j=1,\ldots,K} g_{j}(x) \\
        & \text{where } g_{j}(x) = \frac{\sum_{n=1}^{KQ}\exp(\Omega(x,\xi_{n}^{k})a_{nj} + b_{j})}{\sum_{i=1}^{K} \sum_{n=1}^{KQ}\exp(\Omega(x,\xi_{n}^{k})a_{ni} + b_{i})}
    \end{align*}
    \text{Here $g_{j}$ is the non-linear soft max function}


    \section{Application}\label{sec:application}


    \section{Conclusion}\label{sec:conclusion}
%
%    \subsection{Abbreviations and Acronyms}\label{AA}
%    Define abbreviations and acronyms the first time they are used in the text,
%    even after they have been defined in the abstract. Abbreviations such as
%    IEEE, SI, MKS, CGS, ac, dc, and rms do not have to be defined. Do not use
%    abbreviations in the title or heads unless they are unavoidable.
%
%    \subsection{Units}
%    \begin{itemize}
%        \item Use either SI (MKS) or CGS as primary units. (SI units are encouraged.) English units may be used as secondary units (in parentheses). An exception would be the use of English units as identifiers in trade, such as ``3.5-inch disk drive''.
%        \item Avoid combining SI and CGS units, such as current in amperes and magnetic field in oersteds. This often leads to confusion because equations do not balance dimensionally. If you must use mixed units, clearly state the units for each quantity that you use in an equation.
%        \item Do not mix complete spellings and abbreviations of units: ``Wb/m\textsuperscript{2}'' or ``webers per square meter'', not ``webers/m\textsuperscript{2}''. Spell out units when they appear in text: ``. . . a few henries'', not ``. . . a few H''.
%        \item Use a zero before decimal points: ``0.25'', not ``.25''. Use ``cm\textsuperscript{3}'', not ``cc''.)
%    \end{itemize}
%
%    \subsection{Equations}
%    Number equations consecutively. To make your
%    equations more compact, you may use the solidus (~/~), the exp function, or
%    appropriate exponents. Italicize Roman symbols for quantities and variables,
%    but not Greek symbols. Use a long dash rather than a hyphen for a minus
%    sign. Punctuate equations with commas or periods when they are part of a
%    sentence, as in:
%    \begin{equation}
%        a+b=\gamma\label{eq}
%    \end{equation}
%
%    Be sure that the
%    symbols in your equation have been defined before or immediately following
%    the equation. Use ``\eqref{eq}'', not ``Eq.~\eqref{eq}'' or ``equation \eqref{eq}'', except at
%    the beginning of a sentence: ``Equation \eqref{eq} is . . .''
%
%    \subsection{\LaTeX-Specific Advice}
%
%    Please use ``soft'' (e.g., \verb|\eqref{Eq}|) cross references instead
%    of ``hard'' references (e.g., \verb|(1)|). That will make it possible
%    to combine sections, add equations, or change the order of figures or
%    citations without having to go through the file line by line.
%
%    Please don't use the \verb|{eqnarray}| equation environment. Use
%    \verb|{align}| or \verb|{IEEEeqnarray}| instead. The \verb|{eqnarray}|
%    environment leaves unsightly spaces around relation symbols.
%
%    Please note that the \verb|{subequations}| environment in {\LaTeX}
%    will increment the main equation counter even when there are no
%    equation numbers displayed. If you forget that, you might write an
%    article in which the equation numbers skip from (17) to (20), causing
%    the copy editors to wonder if you've discovered a new method of
%    counting.
%
%        {\BibTeX} does not work by magic. It doesn't get the bibliographic
%    data from thin air but from .bib files. If you use {\BibTeX} to produce a
%    bibliography you must send the .bib files.
%
%        {\LaTeX} can't read your mind. If you assign the same label to a
%    subsubsection and a table, you might find that Table I has been cross
%    referenced as Table IV-B3.
%
%        {\LaTeX} does not have precognitive abilities. If you put a
%    \verb|\label| command before the command that updates the counter it's
%    supposed to be using, the label will pick up the last counter to be
%    cross referenced instead. In particular, a \verb|\label| command
%    should not go before the caption of a figure or a table.
%
%    Do not use \verb|\nonumber| inside the \verb|{array}| environment. It
%    will not stop equation numbers inside \verb|{array}| (there won't be
%    any anyway) and it might stop a wanted equation number in the
%    surrounding equation.
%
%    \subsection{Some Common Mistakes}\label{SCM}
%    \begin{itemize}
%        \item The word ``data'' is plural, not singular.
%        \item The subscript for the permeability of vacuum $\mu_{0}$, and other common scientific constants, is zero with subscript formatting, not a lowercase letter ``o''.
%        \item In American English, commas, semicolons, periods, question and exclamation marks are located within quotation marks only when a complete thought or name is cited, such as a title or full quotation. When quotation marks are used, instead of a bold or italic typeface, to highlight a word or phrase, punctuation should appear outside of the quotation marks. A parenthetical phrase or statement at the end of a sentence is punctuated outside of the closing parenthesis (like this). (A parenthetical sentence is punctuated within the parentheses.)
%        \item A graph within a graph is an ``inset'', not an ``insert''. The word alternatively is preferred to the word ``alternately'' (unless you really mean something that alternates).
%        \item Do not use the word ``essentially'' to mean ``approximately'' or ``effectively''.
%        \item In your paper title, if the words ``that uses'' can accurately replace the word ``using'', capitalize the ``u''; if not, keep using lower-cased.
%        \item Be aware of the different meanings of the homophones ``affect'' and ``effect'', ``complement'' and ``compliment'', ``discreet'' and ``discrete'', ``principal'' and ``principle''.
%        \item Do not confuse ``imply'' and ``infer''.
%        \item The prefix ``non'' is not a word; it should be joined to the word it modifies, usually without a hyphen.
%        \item There is no period after the ``et'' in the Latin abbreviation ``et al.''.
%        \item The abbreviation ``i.e.'' means ``that is'', and the abbreviation ``e.g.'' means ``for example''.
%    \end{itemize}
%    An excellent style manual for science writers is \cite{b7}.
%
%    \subsection{Authors and Affiliations}
%    \textbf{The class file is designed for, but not limited to, six authors.} A
%    minimum of one author is required for all conference articles. Author names
%    should be listed starting from left to right and then moving down to the
%    next line. This is the author sequence that will be used in future citations
%    and by indexing services. Names should not be listed in columns nor group by
%    affiliation. Please keep your affiliations as succinct as possible (for
%    example, do not differentiate among departments of the same organization).
%
%    \subsection{Identify the Headings}
%    Headings, or heads, are organizational devices that guide the reader through
%    your paper. There are two types: component heads and text heads.
%
%    Component heads identify the different components of your paper and are not
%    topically subordinate to each other. Examples include Acknowledgments and
%    References and, for these, the correct style to use is ``Heading 5''. Use
%    ``figure caption'' for your Figure captions, and ``table head'' for your
%    table title. Run-in heads, such as ``Abstract'', will require you to apply a
%    style (in this case, italic) in addition to the style provided by the drop
%    down menu to differentiate the head from the text.
%
%    Text heads organize the topics on a relational, hierarchical basis. For
%    example, the paper title is the primary text head because all subsequent
%    material relates and elaborates on this one topic. If there are two or more
%    sub-topics, the next level head (uppercase Roman numerals) should be used
%    and, conversely, if there are not at least two sub-topics, then no subheads
%    should be introduced.
%
%    \subsection{Figures and Tables}
%
%    \paragraph{Positioning Figures and Tables} Place figures and tables at the top and
%    bottom of columns. Avoid placing them in the middle of columns. Large
%    figures and tables may span across both columns. Figure captions should be
%    below the figures; table heads should appear above the tables. Insert
%    figures and tables after they are cited in the text. Use the abbreviation
%    ``Fig.~\ref{fig}'', even at the beginning of a sentence.
%
%    \begin{table}[htbp]
%        \caption{Table Type Styles}
%        \begin{center}
%            \begin{tabular}{|c|c|c|c|}
%                \hline
%                \textbf{Table} & \multicolumn{3}{|c|}{\textbf{Table Column Head}} \\
%                \cline{2-4}
%                \textbf{Head} & \textbf{\textit{Table column subhead}} & \textbf{\textit{Subhead}} & \textbf{\textit{Subhead}} \\
%                \hline
%                copy          & More table copy$^{\mathrm{a}}$         &                           &                           \\
%                \hline
%                \multicolumn{4}{l}{$^{\mathrm{a}}$Sample of a Table footnote.}
%            \end{tabular}
%            \label{tab1}
%        \end{center}
%    \end{table}
%
%    \begin{figure}[htbp]
%%        \centerline{\includegraphics{fig1.png}}
%        \caption{Example of a figure caption.}
%        \label{fig}
%    \end{figure}
%
%    Figure Labels: Use 8 point Times New Roman for Figure labels. Use words
%    rather than symbols or abbreviations when writing Figure axis labels to
%    avoid confusing the reader. As an example, write the quantity
%    ``Magnetization'', or ``Magnetization, M'', not just ``M''. If including
%    units in the label, present them within parentheses. Do not label axes only
%    with units. In the example, write ``Magnetization (A/m)'' or ``Magnetization
%    \{A[m(1)]\}'', not just ``A/m''. Do not label axes with a ratio of
%    quantities and units. For example, write ``Temperature (K)'', not
%    ``Temperature/K''.
%
%    \section*{Acknowledgment}
%
%    The preferred spelling of the word ``acknowledgment'' in America is without
%    an ``e'' after the ``g''. Avoid the stilted expression ``one of us (R. B.
%    G.) thanks $\ldots$''. Instead, try ``R. B. G. thanks$\ldots$''. Put sponsor
%    acknowledgments in the unnumbered footnote on the first page.
%
%    \section*{References}
%
%    Please number citations consecutively within brackets \cite{b1}. The
%    sentence punctuation follows the bracket \cite{b2}. Refer simply to the reference
%    number, as in \cite{b3}---do not use ``Ref. \cite{b3}'' or ``reference \cite{b3}'' except at
%    the beginning of a sentence: ``Reference \cite{b3} was the first $\ldots$''
%
%    Number footnotes separately in superscripts. Place the actual footnote at
%    the bottom of the column in which it was cited. Do not put footnotes in the
%    abstract or reference list. Use letters for table footnotes.
%
%    Unless there are six authors or more give all authors' names; do not use
%    ``et al.''. Papers that have not been published, even if they have been
%    submitted for publication, should be cited as ``unpublished'' \cite{b4}. Papers
%    that have been accepted for publication should be cited as ``in press'' \cite{b5}.
%    Capitalize only the first word in a paper title, except for proper nouns and
%    element symbols.
%
%    For papers published in translation journals, please give the English
%    citation first, followed by the original foreign-language citation \cite{b6}.
%
%    \begin{thebibliography}{00}
%        \bibitem{b1} G. Eason, B. Noble, and I. N. Sneddon, ``On certain integrals of Lipschitz-Hankel type involving products of Bessel functions,'' Phil. Trans. Roy. Soc. London, vol. A247, pp. 529--551, April 1955.
%        \bibitem{b2} J. Clerk Maxwell, A Treatise on Electricity and Magnetism, 3rd ed., vol. 2. Oxford: Clarendon, 1892, pp.68--73.
%        \bibitem{b3} I. S. Jacobs and C. P. Bean, ``Fine particles, thin films and exchange anisotropy,'' in Magnetism, vol. III, G. T. Rado and H. Suhl, Eds. New York: Academic, 1963, pp. 271--350.
%        \bibitem{b4} K. Elissa, ``Title of paper if known,'' unpublished.
%        \bibitem{b5} R. Nicole, ``Title of paper with only first word capitalized,'' J. Name Stand. Abbrev., in press.
%        \bibitem{b6} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, ``Electron spectroscopy studies on magneto-optical media and plastic substrate interface,'' IEEE Transl. J. Magn. Japan, vol. 2, pp. 740--741, August 1987 [Digests 9th Annual Conf. Magnetics Japan, p. 301, 1982].
%        \bibitem{b7} M. Young, The Technical Writer's Handbook. Mill Valley, CA: University Science, 1989.
%    \end{thebibliography}
%    \vspace{12pt}
%    \color{red}
%    IEEE conference templates contain guidance text for composing and formatting conference papers. Please ensure that all template text is removed from your conference paper prior to submission to the conference. Failure to remove the template text from your paper may result in your paper not being published.

\end{document}
